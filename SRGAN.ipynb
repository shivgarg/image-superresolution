{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SRGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivgarg/image-superresolution/blob/master/SRGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GyQkeM8oejU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torchvision.models import vgg19\n",
        "import math\n",
        "\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        vgg19_model = vgg19(pretrained=True)\n",
        "        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.feature_extractor(img)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(in_features, 0.8),\n",
        "            nn.PReLU(),\n",
        "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(in_features, 0.8),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x)\n",
        "\n",
        "\n",
        "class GeneratorResNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=16):\n",
        "        super(GeneratorResNet, self).__init__()\n",
        "\n",
        "        # First layer\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4), nn.PReLU())\n",
        "\n",
        "        # Residual blocks\n",
        "        res_blocks = []\n",
        "        for _ in range(n_residual_blocks):\n",
        "            res_blocks.append(ResidualBlock(64))\n",
        "        self.res_blocks = nn.Sequential(*res_blocks)\n",
        "\n",
        "        # Second conv layer post residual blocks\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64, 0.8))\n",
        "\n",
        "        # Upsampling layers\n",
        "        upsampling = []\n",
        "        for out_features in range(2):\n",
        "            upsampling += [\n",
        "                # nn.Upsample(scale_factor=2),\n",
        "                nn.Conv2d(64, 256, 3, 1, 1),\n",
        "                nn.BatchNorm2d(256),\n",
        "                nn.PixelShuffle(upscale_factor=2),\n",
        "                nn.PReLU(),\n",
        "            ]\n",
        "        self.upsampling = nn.Sequential(*upsampling)\n",
        "\n",
        "        # Final output layer\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(64, out_channels, kernel_size=9, stride=1, padding=4), nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.conv1(x)\n",
        "        out = self.res_blocks(out1)\n",
        "        out2 = self.conv2(out)\n",
        "        out = torch.add(out1, out2)\n",
        "        out = self.upsampling(out)\n",
        "        out = self.conv3(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.input_shape = input_shape\n",
        "        in_channels, in_height, in_width = self.input_shape\n",
        "        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n",
        "        self.output_shape = (1, patch_h, patch_w)\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, first_block=False):\n",
        "            layers = []\n",
        "            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n",
        "            if not first_block:\n",
        "                layers.append(nn.BatchNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n",
        "            layers.append(nn.BatchNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        layers = []\n",
        "        in_filters = in_channels\n",
        "        for i, out_filters in enumerate([64, 128, 256, 512]):\n",
        "            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n",
        "            in_filters = out_filters\n",
        "\n",
        "        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.model(img)\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import itertools\n",
        "import sys\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torchvision.models import vgg19\n",
        "import math\n",
        "\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        vgg19_model = vgg19(pretrained=True)\n",
        "        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.feature_extractor(img)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(in_features, 0.8),\n",
        "            nn.PReLU(),\n",
        "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(in_features, 0.8),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x)\n",
        "\n",
        "\n",
        "class GeneratorResNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=16):\n",
        "        super(GeneratorResNet, self).__init__()\n",
        "\n",
        "        # First layer\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4), nn.PReLU())\n",
        "\n",
        "        # Residual blocks\n",
        "        res_blocks = []\n",
        "        for _ in range(n_residual_blocks):\n",
        "            res_blocks.append(ResidualBlock(64))\n",
        "        self.res_blocks = nn.Sequential(*res_blocks)\n",
        "\n",
        "        # Second conv layer post residual blocks\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64, 0.8))\n",
        "\n",
        "        # Upsampling layers\n",
        "        upsampling = []\n",
        "        for out_features in range(2):\n",
        "            upsampling += [\n",
        "                # nn.Upsample(scale_factor=2),\n",
        "                nn.Conv2d(64, 256, 3, 1, 1),\n",
        "                nn.BatchNorm2d(256),\n",
        "                nn.PixelShuffle(upscale_factor=2),\n",
        "                nn.PReLU(),\n",
        "            ]\n",
        "        self.upsampling = nn.Sequential(*upsampling)\n",
        "\n",
        "        # Final output layer\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(64, out_channels, kernel_size=9, stride=1, padding=4), nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.conv1(x)\n",
        "        out = self.res_blocks(out1)\n",
        "        out2 = self.conv2(out)\n",
        "        out = torch.add(out1, out2)\n",
        "        out = self.upsampling(out)\n",
        "        out = self.conv3(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.input_shape = input_shape\n",
        "        in_channels, in_height, in_width = self.input_shape\n",
        "        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n",
        "        self.output_shape = (1, patch_h, patch_w)\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, first_block=False):\n",
        "            layers = []\n",
        "            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n",
        "            if not first_block:\n",
        "                layers.append(nn.BatchNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n",
        "            layers.append(nn.BatchNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        layers = []\n",
        "        in_filters = in_channels\n",
        "        for i, out_filters in enumerate([64, 128, 256, 512]):\n",
        "            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n",
        "            in_filters = out_filters\n",
        "\n",
        "        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.model(img)\n",
        "\n",
        "import glob\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Normalization parameters for pre-trained PyTorch models\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root, hr_shape):\n",
        "        hr_height, hr_width = hr_shape\n",
        "        # Transforms for low resolution images and high resolution images\n",
        "        self.lr_transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std),\n",
        "            ]\n",
        "        )\n",
        "        self.hr_transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((hr_height, hr_height), Image.BICUBIC),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.files = sorted(glob.glob(root + \"/*.*\"))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.files[index % len(self.files)])\n",
        "        img_lr = self.lr_transform(img)\n",
        "        img_hr = self.hr_transform(img)\n",
        "\n",
        "        return {\"lr\": img_lr, \"hr\": img_hr}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHkJa1n5puwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "cuda = torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwhOH2Ump8SX",
        "colab_type": "code",
        "outputId": "5498130c-d347-440e-c74d-234a92b5b7d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fX8Hw4q1PX_",
        "colab_type": "code",
        "outputId": "bd2a3369-ea07-4b8e-c194-2b0ed69df714",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!unzip -q /content/drive/My\\ Drive/DL_Seminar/256x256.zip -d /data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace /data/256x256/000000271402.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT4Uoe7-1OQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hr_shape = (256, 256)\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = GeneratorResNet()\n",
        "discriminator = Discriminator(input_shape=(3, *hr_shape))\n",
        "feature_extractor = FeatureExtractor()\n",
        "\n",
        "# Set feature extractor to inference mode\n",
        "feature_extractor.eval()\n",
        "\n",
        "# Losses\n",
        "criterion_GAN = torch.nn.MSELoss()\n",
        "criterion_content = torch.nn.L1Loss()\n",
        "\n",
        "if cuda:\n",
        "    generator = generator.cuda()\n",
        "    discriminator = discriminator.cuda()\n",
        "    feature_extractor = feature_extractor.cuda()\n",
        "    criterion_GAN = criterion_GAN.cuda()\n",
        "    criterion_content = criterion_content.cuda()\n",
        "\n",
        "#if opt.epoch != 0:\n",
        "    # Load pretrained models\n",
        "#    generator.load_state_dict(torch.load(\"saved_models/generator_%d.pth\"))\n",
        "#    discriminator.load_state_dict(torch.load(\"saved_models/discriminator_%d.pth\"))\n",
        "\n",
        "# Optimizers\n",
        "\n",
        "lr = 0.0002\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "batch_size = 4\n",
        "epochs = 200\n",
        "checkpoint_interval = 1\n",
        "\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    ImageDataset(\"/data/256x256\", hr_shape=hr_shape),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO0R73U-6cUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, imgs in enumerate(dataloader):\n",
        "\n",
        "        # Configure model input\n",
        "        imgs_lr = Variable(imgs[\"lr\"].type(Tensor))\n",
        "        imgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(np.ones((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n",
        "        fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n",
        "\n",
        "        # ------------------\n",
        "        #  Train Generators\n",
        "        # ------------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Generate a high resolution image from low resolution input\n",
        "        gen_hr = generator(imgs_lr)\n",
        "\n",
        "        # Adversarial loss\n",
        "        loss_GAN = criterion_GAN(discriminator(gen_hr), valid)\n",
        "\n",
        "        # Content loss\n",
        "        gen_features = feature_extractor(gen_hr)\n",
        "        real_features = feature_extractor(imgs_hr)\n",
        "        loss_content = criterion_content(gen_features, real_features.detach())\n",
        "\n",
        "        # Total loss\n",
        "        loss_G = loss_content + 1e-3 * loss_GAN\n",
        "\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Loss of real and fake images\n",
        "        loss_real = criterion_GAN(discriminator(imgs_hr), valid)\n",
        "        loss_fake = criterion_GAN(discriminator(gen_hr.detach()), fake)\n",
        "\n",
        "        # Total loss\n",
        "        loss_D = (loss_real + loss_fake) / 2\n",
        "\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # --------------\n",
        "        #  Log Progress\n",
        "        # --------------\n",
        "\n",
        "  \n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        if batches_done % 200 == 0:\n",
        "            sys.stdout.write(\n",
        "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\\n\"\n",
        "            % (epoch, epochs, i, len(dataloader), loss_D.item(), loss_G.item())\n",
        "        )\n",
        "\n",
        "            # Save image grid with upsampled inputs and SRGAN outputs\n",
        "            imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\n",
        "            gen_hr = make_grid(gen_hr, nrow=1, normalize=True)\n",
        "            imgs_lr = make_grid(imgs_lr, nrow=1, normalize=True)\n",
        "            img_grid = torch.cat((imgs_lr, gen_hr), -1)\n",
        "            import matplotlib.pyplot as plt\n",
        "            image = np.moveaxis(img_grid.detach().cpu().numpy(),0,-1)\n",
        "            plt.imshow(image)\n",
        "            plt.show()\n",
        "             \n",
        "    if epoch % checkpoint_interval == 0:\n",
        "        # Save model checkpoints\n",
        "        torch.save(generator.state_dict(), \"/content/drive/My Drive/DL_Seminar/ckpts/generator_%d.pth\" % epoch)\n",
        "        torch.save(discriminator.state_dict(), \"/content/drive/My Drive/DL_Seminar/ckptsdiscriminator_%d.pth\" % epoch)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}